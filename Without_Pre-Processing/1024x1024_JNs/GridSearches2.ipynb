{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2585f149-755c-45dc-b1d3-e94e37b2f972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Alexandra\\anaconda3\\envs\\arenvtools\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Alexandra\\anaconda3\\envs\\arenvtools\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Alexandra\\anaconda3\\envs\\arenvtools\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 1022, 1022, 32)    320       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 1022, 1022, 32)    128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 511, 511, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 509, 509, 64)      18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 509, 509, 64)      256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 254, 254, 64)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 252, 252, 128)     73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 252, 252, 128)     512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 126, 126, 128)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2032128)           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               520225024 \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                8224      \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 520328067 (1.94 GB)\n",
      "Trainable params: 520327043 (1.94 GB)\n",
      "Non-trainable params: 1024 (4.00 KB)\n",
      "_________________________________________________________________\n",
      "Found 186 images belonging to 3 classes.\n",
      "Found 63 images belonging to 3 classes.\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Alexandra\\anaconda3\\envs\\arenvtools\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Alexandra\\anaconda3\\envs\\arenvtools\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "5/5 [==============================] - 40s 8s/step - loss: 1.6699 - accuracy: 0.3688 - val_loss: 1.1240 - val_accuracy: 0.2812\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 35s 7s/step - loss: 1.0676 - accuracy: 0.5000 - val_loss: 1.1106 - val_accuracy: 0.3750\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 36s 7s/step - loss: 0.9356 - accuracy: 0.5909 - val_loss: 1.0864 - val_accuracy: 0.3750\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 36s 7s/step - loss: 0.9010 - accuracy: 0.5844 - val_loss: 1.1033 - val_accuracy: 0.3125\n",
      "Epoch 5/10\n",
      "4/5 [=======================>......] - ETA: 6s - loss: 0.8061 - accuracy: 0.5938 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define paths\n",
    "dataset_folder = 'C:\\\\Users\\\\Alexandra\\\\Documents\\\\SPRING_2024\\\\BMEN_689\\\\Project_2\\\\Jupyter_Notebook\\\\1024x1024_Phage_EMs\\\\'\n",
    "\n",
    "# Function to split dataset into train/validation/test\n",
    "def split_dataset(data_folder, train_size=0.6, val_size=0.2, test_size=0.2):\n",
    "    for class_folder in os.listdir(data_folder):\n",
    "        class_path = os.path.join(data_folder, class_folder)\n",
    "        if os.path.isdir(class_path):\n",
    "            files = os.listdir(class_path)\n",
    "            train_files, test_files = train_test_split(files, test_size=val_size+test_size, random_state=42)\n",
    "            val_files, test_files = train_test_split(test_files, test_size=test_size/(val_size+test_size), random_state=42)\n",
    "\n",
    "            train_folder = os.path.join(data_folder, 'train', class_folder)\n",
    "            val_folder = os.path.join(data_folder, 'validation', class_folder)\n",
    "            test_folder = os.path.join(data_folder, 'test', class_folder)\n",
    "            for folder in [train_folder, val_folder, test_folder]:\n",
    "                if not os.path.exists(folder):\n",
    "                    os.makedirs(folder)\n",
    "\n",
    "            for file in train_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(train_folder, file))\n",
    "            for file in val_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(val_folder, file))\n",
    "            for file in test_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(test_folder, file))\n",
    "\n",
    "split_dataset(dataset_folder)\n",
    "\n",
    "def create_condensed_cnn(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),,\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.1),        \n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Adjusted for grayscale images\n",
    "input_shape = (1024, 1024, 1)\n",
    "num_classes = 3\n",
    "\n",
    "model = create_condensed_cnn(input_shape, num_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Define directory paths\n",
    "train_dir = os.path.join(dataset_folder, 'train')\n",
    "validation_dir = os.path.join(dataset_folder, 'validation')\n",
    "test_dir = os.path.join(dataset_folder, 'test')\n",
    "\n",
    "# ImageDataGenerator with normalization\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 32\n",
    "target_size = (1024, 1024)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e174d8-b94d-4416-9254-ca582185d603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Alexandra\\anaconda3\\envs\\arenvtools\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Alexandra\\anaconda3\\envs\\arenvtools\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Alexandra\\anaconda3\\envs\\arenvtools\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Alexandra\\anaconda3\\envs\\arenvtools\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 1022, 1022, 32)    320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 511, 511, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 509, 509, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 254, 254, 64)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 252, 252, 128)     73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 126, 126, 128)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 124, 124, 256)     295168    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 62, 62, 256)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 60, 60, 512)       1180160   \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 30, 30, 512)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 460800)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               117965056 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119576387 (456.15 MB)\n",
      "Trainable params: 119576387 (456.15 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Found 186 images belonging to 3 classes.\n",
      "Found 63 images belonging to 3 classes.\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Alexandra\\anaconda3\\envs\\arenvtools\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Alexandra\\anaconda3\\envs\\arenvtools\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "5/5 [==============================] - 31s 6s/step - loss: 1.4865 - accuracy: 0.3182 - val_loss: 1.0978 - val_accuracy: 0.3125\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 29s 6s/step - loss: 1.1185 - accuracy: 0.3636 - val_loss: 1.1063 - val_accuracy: 0.2500\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 29s 6s/step - loss: 1.1020 - accuracy: 0.2727 - val_loss: 1.0918 - val_accuracy: 0.4375\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 30s 6s/step - loss: 1.1007 - accuracy: 0.3117 - val_loss: 1.0956 - val_accuracy: 0.4688\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 30s 6s/step - loss: 1.1000 - accuracy: 0.3117 - val_loss: 1.0942 - val_accuracy: 0.4375\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 30s 6s/step - loss: 1.0977 - accuracy: 0.3247 - val_loss: 1.0700 - val_accuracy: 0.3438\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 32s 6s/step - loss: 1.0941 - accuracy: 0.4000 - val_loss: 1.0953 - val_accuracy: 0.4375\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 31s 6s/step - loss: 1.0861 - accuracy: 0.4416 - val_loss: 1.0148 - val_accuracy: 0.3750\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 32s 6s/step - loss: 1.0493 - accuracy: 0.4091 - val_loss: 1.0193 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 30s 6s/step - loss: 1.0735 - accuracy: 0.3961 - val_loss: 1.0283 - val_accuracy: 0.4688\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define paths\n",
    "dataset_folder = 'C:\\\\Users\\\\Alexandra\\\\Documents\\\\SPRING_2024\\\\BMEN_689\\\\Project_2\\\\Jupyter_Notebook\\\\1024x1024_Phage_EMs\\\\'\n",
    "\n",
    "# Function to split dataset into train/validation/test\n",
    "def split_dataset(data_folder, train_size=0.6, val_size=0.2, test_size=0.2):\n",
    "    for class_folder in os.listdir(data_folder):\n",
    "        class_path = os.path.join(data_folder, class_folder)\n",
    "        if os.path.isdir(class_path):\n",
    "            files = os.listdir(class_path)\n",
    "            train_files, test_files = train_test_split(files, test_size=val_size+test_size, random_state=42)\n",
    "            val_files, test_files = train_test_split(test_files, test_size=test_size/(val_size+test_size), random_state=42)\n",
    "\n",
    "            train_folder = os.path.join(data_folder, 'train', class_folder)\n",
    "            val_folder = os.path.join(data_folder, 'validation', class_folder)\n",
    "            test_folder = os.path.join(data_folder, 'test', class_folder)\n",
    "            for folder in [train_folder, val_folder, test_folder]:\n",
    "                if not os.path.exists(folder):\n",
    "                    os.makedirs(folder)\n",
    "\n",
    "            for file in train_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(train_folder, file))\n",
    "            for file in val_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(val_folder, file))\n",
    "            for file in test_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(test_folder, file))\n",
    "\n",
    "split_dataset(dataset_folder)\n",
    "\n",
    "def create_condensed_cnn(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(512, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.1), \n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Adjusted for grayscale images\n",
    "input_shape = (1024, 1024, 1)\n",
    "num_classes = 3\n",
    "\n",
    "model = create_condensed_cnn(input_shape, num_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Define directory paths\n",
    "train_dir = os.path.join(dataset_folder, 'train')\n",
    "validation_dir = os.path.join(dataset_folder, 'validation')\n",
    "test_dir = os.path.join(dataset_folder, 'test')\n",
    "\n",
    "# ImageDataGenerator with normalization\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 32\n",
    "target_size = (1024, 1024)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619807e-9b5c-49c4-b7aa-79486c7abc7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
